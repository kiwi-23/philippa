# Philippa
<p><b>An artistic exploration of a philosophical inquiry regarding the worth of a person’s life, inspecting the grounds of moral status and mapping it onto a logical framework to build an intelligent system that is capable of measuring morality.</b></p>

<br>
<h2>Abstract</h2>

You are the driver of a trolley car that is hurtling down the track at sixty miles an hour, and you notice there are five workers working towards the end of the track. The brakes are faulty, and you feel desperate because you know that if the trolley was to crash into these people, they will surely die.

You feel helpless until you notice that there is a sidetrack off to the right, and only one worker is working on that track. Your steering wheel works, so you can turn the trolley car, if you want to, onto this sidetrack- killing the one worker, but sparing five.

<i>Which is the more ethical option?</i>

• To do nothing and kill the five workers working on the track.

• Or to steer the trolley car onto the sidetrack and kill the one individual worker.

This problem illustrates two kinds of moral reasonings- <a href="https://en.wikipedia.org/wiki/Consequentialism">Consequentialism</a> that bases the morality of an action on the consequences of the action, and <a href="https://en.wikipedia.org/wiki/Deontological_ethics">Deontology</a> that bases the morality on the nature of the action.

Consequentialists would argue that it would be morally unethical to kill five when you can kill one, that it should always be one’s goal to consider the greatest good for the greatest number. On the other hand, Deontology states that murder is murder, no matter the consequences.

This thought enthused my exploration of morality, specifically into trying to evaluate the worth of a person’s life. I inspect the moral worth of beings with respect to personhood, and try to identify the grounds for the heirarchy of moral statuses assigned to different entities on the basis of rationality and sentience.

The goal is to dissect our moral codes and build a framework where logic meets morality. I fabricate a system that is able to understand these moral variables and make decisions beyond what the scope of a simple quantitative comparision would allow.

<br>
<h2>Survey</h2>

In my inquiry, <a href="https://docs.google.com/forms/d/e/1FAIpQLSelGcMKG3RO2SQF9nmVPNPvnKCHpN9_e6N-ZoyUUNCIMpvNOg/viewform">I interviewed a large number of people</a> and proposed 5 such scenarios to them, other than the trolley problem, here are the four other variations:

<ol>

<li>
You are a transplant surgeon, you have 5 patients in critical need of an organ each. One needs a liver, two need a kidney,
one a heart, another a lung. There are no donors. But there is a healthy patient in the next room. Which is the more ethical course of action?

• Use the healthy patient’s organs to save 5 lives

• Do nothing & let the 5 die</li>

<li>You are a judge, and 5 activists are threatening to kill themselves if the culprit of a certain crime is not found. The real culprit remains unknown. You can prevent the bloodshed by framing an innocent individual. Which is the more ethical course of action?

• Execute the innocent person & save 5 lives

• Do nothing & let the 5 die</li>

<li>You are a doctor, and in order to save a patient, you need to give him a massive dose of a drug that’s in short supply. A little later, 5 other patients arrive, each of whom could be saved with 1/5th of that dose. Which is the more ethical option?

• Give the dose to the 5 later patients

• Give the dose to the initial patient</li>

<li>You are a doctor, and there are 5 patients whose lives could be saved by the manufacture of a certain gas. But this leads to the release of lethal fumes into the room of another patient who, for some reason, you cannot move elsewhere. Which is the more ethical option?

• Manufacture the gas & let the 1 patient die

• Do nothing & let the 5 patients die</li></ol>

<br>
<h2>Methodology</h2>

The 5 scenarios all use different combinations of four kinds of moral variables: number, desire, intent and duty.

<br>
The moral framework resembles this:

action [(<i>number</i>), (<i>desire</i>), (<i>intent</i>), (<i>duty</i>)], where

<b>number</b> = the number of lives saved by that action,

<b>desire</b> = the kind of desire that’s fulfilled by that action,

<b>intent</b> = our intention behind the action, and

<b>duty</b> = the kind of obligation we’re adhering to.

<br>
“Philippa” is a neural network that is trained on this crowdsourced data. She has learnt to make ethical choices beyond what the arithmetic scope of (5 lives > 1 life) allows. 

<br>
<h2>Other Links</h2>

<a href="https://mya-kiwi.com/project/philippa">web portal</a> / <a href="http://dl.dropboxusercontent.com/s/juhp5gzxh042bx7/philippa-doc.pdf?dl=0">documentation</a>
